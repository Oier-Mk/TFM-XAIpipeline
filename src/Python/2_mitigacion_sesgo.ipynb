{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AIF360: Fairness in AI Toolkit\n",
    "\n",
    "AIF360 es un conjunto de herramientas diseñadas para abordar la equidad y la justicia en los sistemas de inteligencia artificial (IA). Ofrece una serie de algoritmos, métricas y utilidades para ayudar a los desarrolladores y los profesionales de IA a comprender, medir y mitigar los sesgos y las disparidades en los modelos de IA.\n",
    "\n",
    "## Utilización\n",
    "\n",
    "- **Evaluar la equidad:** Proporciona métricas predefinidas y personalizables para evaluar la equidad de los modelos de IA en función de atributos sensibles como género, raza, edad, etc.\n",
    "- **Mitigar sesgos:** Ofrece algoritmos y técnicas para mitigar los sesgos identificados en los modelos de IA, ya sea en el preprocesamiento de datos, durante el entrenamiento del modelo o en la etapa de postprocesamiento.\n",
    "- **Comprender el impacto de las decisiones de IA:** Permite a los usuarios comprender cómo los modelos de IA pueden afectar a diferentes grupos demográficos y cómo esos impactos pueden ser injustos o discriminatorios.\n",
    "- **Promover la equidad y la justicia:** Facilita el diseño de sistemas de IA más equitativos y justos al proporcionar herramientas para identificar y abordar los sesgos, lo que lleva a una toma de decisiones más inclusiva y ética."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Librerias requeridas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:\n",
      "`load_boston` has been removed from scikit-learn since version 1.2.\n",
      "\n",
      "The Boston housing prices dataset has an ethical problem: as\n",
      "investigated in [1], the authors of this dataset engineered a\n",
      "non-invertible variable \"B\" assuming that racial self-segregation had a\n",
      "positive impact on house prices [2]. Furthermore the goal of the\n",
      "research that led to the creation of this dataset was to study the\n",
      "impact of air quality but it did not give adequate demonstration of the\n",
      "validity of this assumption.\n",
      "\n",
      "The scikit-learn maintainers therefore strongly discourage the use of\n",
      "this dataset unless the purpose of the code is to study and educate\n",
      "about ethical issues in data science and machine learning.\n",
      "\n",
      "In this special case, you can fetch the dataset from the original\n",
      "source::\n",
      "\n",
      "    import pandas as pd\n",
      "    import numpy as np\n",
      "\n",
      "    data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
      "    raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
      "    data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
      "    target = raw_df.values[1::2, 2]\n",
      "\n",
      "Alternative datasets include the California housing dataset and the\n",
      "Ames housing dataset. You can load the datasets as follows::\n",
      "\n",
      "    from sklearn.datasets import fetch_california_housing\n",
      "    housing = fetch_california_housing()\n",
      "\n",
      "for the California housing dataset and::\n",
      "\n",
      "    from sklearn.datasets import fetch_openml\n",
      "    housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
      "\n",
      "for the Ames housing dataset.\n",
      "\n",
      "[1] M Carlisle.\n",
      "\"Racist data destruction?\"\n",
      "<https://medium.com/@docintangible/racist-data-destruction-113e3eff54a8>\n",
      "\n",
      "[2] Harrison Jr, David, and Daniel L. Rubinfeld.\n",
      "\"Hedonic housing prices and the demand for clean air.\"\n",
      "Journal of environmental economics and management 5.1 (1978): 81-102.\n",
      "<https://www.researchgate.net/publication/4974606_Hedonic_housing_prices_and_the_demand_for_clean_air>\n",
      ": LawSchoolGPADataset will be unavailable. To install, run:\n",
      "pip install 'aif360[LawSchoolGPA]'\n"
     ]
    }
   ],
   "source": [
    "from aif360.metrics import BinaryLabelDatasetMetric\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "from aif360.algorithms.preprocessing.reweighing import Reweighing\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from IPython.display import Markdown, display\n",
    "from aif360.algorithms.preprocessing.lfr import LFR\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lectura del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/Statlog_preprocesado.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### El AIF360 requiere de un dataset de tipo numérico para poder trabajar, por lo que se realiza una conversión de las variables categóricas a numéricas.\n",
    "\n",
    "Se realiza un label encoder porque se ha comprobado que con one hot encoder el modelo pierde precisión en los pesos. Solamente se utilizará este tipo de encoding en este notebook para poder trabajar con el AIF360, posteriormente se volverá a utilizar el dataset original con los pesos obtenidos con AIF360."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Status of existing checking account\n",
      "mapping {'0 - 200 DM': 0, '< 0 DM': 1, '>= 200 DM': 2, 'no checking account': 3}\n",
      "Feature Credit history\n",
      "mapping {'all credits paid': 0, 'critical account': 1, 'delay in paying': 2, 'existing credits paid': 3, 'no credits/all paid': 4}\n",
      "Feature Purpose\n",
      "mapping {'business': 0, 'car (new)': 1, 'car (used)': 2, 'domestic appliances': 3, 'education': 4, 'furniture/equipment': 5, 'others': 6, 'radio/television': 7, 'repairs': 8, 'retraining': 9}\n",
      "Feature Savings account/bonds\n",
      "mapping {'100 - 500 DM': 0, '500 - 1000 DM': 1, '< 100 DM': 2, '>= 1000 DM': 3, 'unknown/no savings': 4}\n",
      "Feature Present employment since\n",
      "mapping {'1 - 4 years': 0, '4 - 7 years': 1, '< 1 year': 2, '>= 7 years': 3, 'unemployed': 4}\n",
      "Feature Other debtors / guarantors\n",
      "mapping {'co-applicant': 0, 'guarantor': 1, 'none': 2}\n",
      "Feature Property\n",
      "mapping {'building society savings': 0, 'car or other': 1, 'real estate': 2, 'unknown/no property': 3}\n",
      "Feature Other installment plans\n",
      "mapping {'bank': 0, 'none': 1, 'stores': 2}\n",
      "Feature Housing\n",
      "mapping {'for free': 0, 'own': 1, 'rent': 2}\n",
      "Feature Job\n",
      "mapping {'management/self-employed': 0, 'skilled employee': 1, 'unemployed/non-resident': 2, 'unskilled resident': 3}\n",
      "Feature Telephone\n",
      "mapping {'none': 0, 'yes, registered': 1}\n",
      "Feature Foreign worker\n",
      "mapping {'no': 0, 'yes': 1}\n",
      "Feature Gender\n",
      "mapping {'female': 0, 'male': 1}\n",
      "Feature Marital Status\n",
      "mapping {'divorced/separated': 0, 'divorced/separated/married': 1, 'married/widowed': 2, 'single': 3}\n"
     ]
    }
   ],
   "source": [
    "categorical_columns = df.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "data_encoded = df.copy(deep=True)\n",
    "\n",
    "#Use Scikit-learn label encoding to encode character data\n",
    "lab_enc = preprocessing.LabelEncoder()\n",
    "for col in categorical_columns:\n",
    "        data_encoded[col] = lab_enc.fit_transform(df[col])\n",
    "        le_name_mapping = dict(zip(lab_enc.classes_, lab_enc.transform(lab_enc.classes_)))\n",
    "        print('Feature', col)\n",
    "        print('mapping', le_name_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Se realiza un split del dataset en train y test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_encoded\n",
    "X = X.drop('Class', axis=1)\n",
    "y = data_encoded['Class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Se convierte el dataframe de Pandas en un objeto BinaryLabelDataset para poder trabajar con AIF360"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es necesario definir ciertos parámetros para poder realizar este tipo de conjunto de datos:\n",
    "\n",
    "1. Asignación de variable objetivo y sus valores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_label = ['Class']\n",
    "favorable_labels = 1  # 'Concedido'\n",
    "unfavorable_labels = 2   # 'Denegado'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Asignación de variables protegidas y sus valores\n",
    "\n",
    "Estas son variables que pueden generar algún sesgo en la sociedad como la edad, el genero, la raza..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "protected_attributes = [\n",
    "    'Gender',\n",
    "    'Foreign worker',\n",
    "    'Marital Status',\n",
    "    'Job'\n",
    "]\n",
    "\n",
    "privileged_groups = [\n",
    "    {\n",
    "        'Gender': 1, # Hombre\n",
    "        'Foreign worker': 0, # 'no'\n",
    "        'Marital Status': 2, # 'married'\n",
    "        'Job': 0, # 'management/self-employed'\n",
    "        'Job': 1, # 'skilled employee'\n",
    "     }\n",
    "]\n",
    "\n",
    "unprivileged_groups = [\n",
    "    {\n",
    "        'Gender': 0,  # Mujer\n",
    "        'Foreign worker': 1, # 'yes'\n",
    "        'Marital Status': 0,  # 'divorced/separated'\n",
    "        'Marital Status': 3,  # 'single'\n",
    "        'Marital Status': 1,   # 'divorced/separated/married'\n",
    "        'Job': 2,  # 'unemployed/non-resident'\n",
    "        'Job': 3   # 'unskilled resident'\n",
    "    }  \n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso se eligen las siguientes variables como protegidas:\n",
    "- Genero\n",
    "- Trabajador extranjero (se entiende que es inmigrante)\n",
    "- Estado civil\n",
    "- Trabajo (Se entiende que tener _skills_ va directamente relacionado con el nivel de estudios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_dataset = BinaryLabelDataset(df=data_encoded,\n",
    "                                      label_names=target_label,\n",
    "                                      favorable_label=favorable_labels,\n",
    "                                      unfavorable_label=unfavorable_labels,\n",
    "                                      protected_attribute_names=protected_attributes\n",
    "                                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Métricas de equidad del conjunto de datos original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diferencia de resultados medios entre grupos privilegiados y no privilegiados = -0.093750\n"
     ]
    }
   ],
   "source": [
    "# Metric for the original dataset\n",
    "original_bias = BinaryLabelDatasetMetric(binary_dataset, \n",
    "                                             unprivileged_groups=unprivileged_groups,\n",
    "                                             privileged_groups=privileged_groups)\n",
    "print(\"Diferencia de resultados medios entre grupos privilegiados y no privilegiados = %f\" % original_bias.mean_difference())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformación del conjunto de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existen 4 tipos de transformaciones que se pueden realizar en el conjunto de datos:\n",
    "- **Reweighing:** Modifica los pesos de las instancias para que el conjunto de datos sea más equitativo.\n",
    "- **Disparate Impact Remover:** Elimina las características que pueden generar sesgo en el conjunto de datos.\n",
    "- **LFR (Learning Fair Representations):** Aprende una representación justa del conjunto de datos.\n",
    "- **Data preprocessing:** Realiza un preprocesamiento de los datos para eliminar el sesgo.\n",
    "\n",
    "Se ha comprobado que _Data preprocessing_ tarda demasiado como para poder realizarlo y _Dispare impact remover_ necesita el modelo entre medias y no es valido en este caso, por lo que se realizará una transformación con _Reweighing_ y _Disparate Impact Remover_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "RW = Reweighing(unprivileged_groups=unprivileged_groups,\n",
    "               privileged_groups=privileged_groups)\n",
    "RW.fit(binary_dataset)\n",
    "rw_binary_transformed = RW.transform(binary_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, loss: 168204.53899711833, L_x: 1682038.5979889648,  L_y: 0.6790950755325048,  L_z: 5.157316949671448e-05\n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =          220     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  1.68205D+05    |proj g|=  1.37799D+01\n",
      "step: 250, loss: 168180.1846876671, L_x: 1681796.5711957058,  L_y: 0.5274749998875315,  L_z: 4.654830090078874e-05\n",
      "step: 500, loss: 168049.32496008798, L_x: 1680493.528727489,  L_y: -0.027965267161592543,  L_z: 2.630310682571451e-05\n",
      "step: 750, loss: 167411.57904698554, L_x: 1674130.7933263965,  L_y: -1.500288695220256,  L_z: 1.5205490084642702e-06\n",
      "step: 1000, loss: 167355.01670817164, L_x: 1673566.6175087118,  L_y: -1.6450451403795292,  L_z: 1.2204040867598533e-06\n",
      "\n",
      "At iterate    1    f=  1.67355D+05    |proj g|=  6.49744D+01\n",
      "  ys=-3.455E+02  -gs= 5.058E+02 BFGS update SKIPPED\n",
      "step: 1250, loss: 163483.12300249483, L_x: 1634939.3609851154,  L_y: -10.813096016735145,  L_z: 3.4034019120821506e-34\n",
      "\n",
      "At iterate    2    f=  1.63483D+05    |proj g|=  5.87279D+01\n",
      "step: 1500, loss: 163482.94213850203, L_x: 1634937.5523451874,  L_y: -10.813096016735145,  L_z: 3.3930193351203385e-34\n",
      "step: 1750, loss: 163482.21868433713, L_x: 1634930.3178035384,  L_y: -10.813096016735145,  L_z: 3.351804834801243e-34\n",
      "step: 2000, loss: 163467.75020795246, L_x: 1634785.6330396917,  L_y: -10.813096016735145,  L_z: 2.6249912196574786e-34\n",
      "step: 2250, loss: 163421.45885172227, L_x: 1634322.71947739,  L_y: -10.813096016735145,  L_z: 1.2007791280473502e-34\n",
      "step: 2500, loss: 163236.41180532277, L_x: 1632472.2490133948,  L_y: -10.813096016735145,  L_z: 5.2615990965970196e-36\n",
      "step: 2750, loss: 162498.11765141427, L_x: 1625089.30747431,  L_y: -10.813096016735145,  L_z: 1.972210935645087e-41\n",
      "step: 3000, loss: 159575.245561596, L_x: 1595860.5865761272,  L_y: -10.813096016735145,  L_z: 7.508315956536873e-63\n",
      "step: 3250, loss: 159158.54016749762, L_x: 1591693.5326351435,  L_y: -10.813096016735145,  L_z: 7.371299874489181e-66\n",
      "\n",
      "At iterate    3    f=  1.59159D+05    |proj g|=  5.73083D+01\n",
      "step: 3500, loss: 62629.85806628881, L_x: 626370.0481575502,  L_y: -7.191039253088837,  L_z: 0.022144893440897387\n",
      "\n",
      "At iterate    4    f=  6.26299D+04    |proj g|=  1.20558D+01\n",
      "step: 3750, loss: 54063.21318410128, L_x: 540694.2542202757,  L_y: -6.2605996427600115,  L_z: 0.024180858236481462\n",
      "\n",
      "At iterate    5    f=  5.40632D+04    |proj g|=  9.78885D+00\n",
      "step: 4000, loss: 49264.156209362925, L_x: 492695.0482679716,  L_y: -5.389640453919999,  L_z: 0.020511509838978904\n",
      "\n",
      "At iterate    6    f=  4.92642D+04    |proj g|=  6.08758D+00\n",
      "step: 4250, loss: 47712.310866209125, L_x: 477169.2487750498,  L_y: -4.644335121649252,  L_z: 0.015161912897051188\n",
      "\n",
      "At iterate    7    f=  4.77123D+04    |proj g|=  5.60308D+00\n",
      "step: 4500, loss: 47961.13222480209, L_x: 479660.4086946029,  L_y: -4.943578105105449,  L_z: 0.017466723450127494\n",
      "step: 4750, loss: 47665.2665970252, L_x: 476699.1289905885,  L_y: -4.680236204843322,  L_z: 0.016967085593690527\n",
      "\n",
      "At iterate    8    f=  4.76653D+04    |proj g|=  5.74513D+00\n",
      "step: 5000, loss: 47890.94317654958, L_x: 478957.7962782816,  L_y: -4.870934157069256,  L_z: 0.01724143924764448\n",
      "step: 5250, loss: 47705.71852722604, L_x: 477103.70654553256,  L_y: -4.687258507307216,  L_z: 0.017565590041183708\n",
      "step: 5500, loss: 47722.5960347127, L_x: 477272.4498594394,  L_y: -4.68374455092001,  L_z: 0.01739665983735155\n",
      "step: 5750, loss: 47665.25592308188, L_x: 476699.024151386,  L_y: -4.680454819903254,  L_z: 0.016981381589617463\n",
      "step: 6000, loss: 47665.12536441313, L_x: 476697.7180671488,  L_y: -4.68039102877125,  L_z: 0.016974363508616405\n",
      "\n",
      "At iterate    9    f=  4.76651D+04    |proj g|=  6.53661D+00\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "  220      9     28     13     1     5   6.537D+00   4.767D+04\n",
      "  F =   47665.125363770225     \n",
      "\n",
      "STOP: TOTAL NO. of f AND g EVALUATIONS EXCEEDS LIMIT        \n"
     ]
    }
   ],
   "source": [
    "TR = LFR(unprivileged_groups=unprivileged_groups,\n",
    "         privileged_groups=privileged_groups,\n",
    "         k=10, Ax=0.1, Ay=1.0, Az=2.0,\n",
    "         verbose=1\n",
    "        )\n",
    "TR = TR.fit(binary_dataset, maxiter=5000, maxfun=5000)\n",
    "tr_binary_transformed = TR.transform(binary_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Métricas de equidad del conjunto de datos tras transformar los pesos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diferencia de resultados medios entre grupos privilegiados y no privilegiados con RW = 0.000000\n"
     ]
    }
   ],
   "source": [
    "rw_transformed_bias = BinaryLabelDatasetMetric(rw_binary_transformed, \n",
    "                                         unprivileged_groups=unprivileged_groups,\n",
    "                                         privileged_groups=privileged_groups)\n",
    "print(\"Diferencia de resultados medios entre grupos privilegiados y no privilegiados con RW = %f\" % rw_transformed_bias.mean_difference())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diferencia de resultados medios entre grupos privilegiados y no privilegiados con TR = 0.000000\n"
     ]
    }
   ],
   "source": [
    "tr_transformed_bias = BinaryLabelDatasetMetric(tr_binary_transformed, \n",
    "                                         unprivileged_groups=unprivileged_groups,\n",
    "                                         privileged_groups=privileged_groups)\n",
    "print(\"Diferencia de resultados medios entre grupos privilegiados y no privilegiados con TR = %f\" % tr_transformed_bias.mean_difference())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Con el dataset con pesos REWEIGHING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the dataset and split into train and test\n",
    "transformed_train, transformed_test = rw_binary_transformed.split([0.8], shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 70.71\n"
     ]
    }
   ],
   "source": [
    "# Logistic regression classifier and predictions\n",
    "b_X_train = transformed_train.features\n",
    "b_y_train = transformed_train.labels.ravel()\n",
    "b_w_train = transformed_train.instance_weights.ravel()\n",
    "\n",
    "b_X_test = transformed_test.features\n",
    "b_y_test = transformed_test.labels.ravel()\n",
    "b_w_test = transformed_test.instance_weights.ravel() \n",
    "\n",
    "model = LogisticRegression(max_iter=1000000)\n",
    "\n",
    "model.fit(b_X_train, b_y_train, sample_weight=b_w_train)\n",
    "\n",
    "b_y_pred = model.predict(b_X_test)\n",
    "\n",
    "accuracy = accuracy_score(b_y_test, b_y_pred, sample_weight=b_w_test) \n",
    "print('Accuracy: %.2f' % (accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Con el dataset con pesos LFR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the dataset and split into train and test\n",
    "tr_X = tr_binary_transformed.convert_to_dataframe()[0]\n",
    "tr_y = tr_X.get('Class')\n",
    "tr_X = tr_X.drop('Class', axis=1)\n",
    "b_X_train, b_X_test, b_y_train, b_y_test = train_test_split(tr_X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 71.00\n"
     ]
    }
   ],
   "source": [
    "# Logistic regression classifier and predictions\n",
    "\n",
    "model = LogisticRegression(max_iter=1000000)\n",
    "\n",
    "model.fit(b_X_train, b_y_train)\n",
    "\n",
    "b_y_pred = model.predict(b_X_test)\n",
    "\n",
    "accuracy = accuracy_score(b_y_test, b_y_pred) \n",
    "print('Accuracy: %.2f' % (accuracy * 100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Con el dataset sin cambios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 76.00\n"
     ]
    }
   ],
   "source": [
    "# read the original csv\n",
    "df = pd.read_csv('../data/Statlog_preprocesado.csv')\n",
    "\n",
    "categorical_column = df.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "data_encoded = df.copy(deep=True)\n",
    "#Use Scikit-learn label encoding to encode character data\n",
    "lab_enc = preprocessing.LabelEncoder()\n",
    "for col in categorical_column:\n",
    "        data_encoded[col] = lab_enc.fit_transform(df[col])\n",
    "        le_name_mapping = dict(zip(lab_enc.classes_, lab_enc.transform(lab_enc.classes_)))\n",
    "\n",
    "model = LogisticRegression(max_iter=1000000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy: %.2f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guardar los pesos\n",
    "\n",
    "Se ha demostrado que el modelo con pesos REWEIGHING es el que mejor se ajusta a los datos, por lo que se guardan los pesos obtenidos para poder utilizarlos en el modelo final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to a csv binary_transformed.instance_weights\n",
    "df['Weights'] = rw_binary_transformed.instance_weights\n",
    "df.to_csv('../data/Statlog_weights.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
